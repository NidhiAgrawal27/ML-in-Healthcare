\section{Models and Methods}
\label{sec:models_and_methods}
\subsection{Dataset}
We evaluate our models on an MRI imaging dataset and a dataset consisting of radiomics features extracted from these images:

\begin{itemize}[leftmargin=0cm]
    \setlength\itemsep{0.6em}
    \item[]
    \textbf{MRI images dataset:} The dataset consists images of 278 brain slices, 111 with tumor and 167 without any tumor, taken from the Kaggle datasets: Brain MRI Images for Brain Tumor Detection\footnote{\url{https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection}} and Brain Tumor Classification (MRI)\footnote{\url{https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri}}. Expert knowledge is not required to see the tumors in the images as they are clearly visible. The dataset is split into $250$ ($\sim$89\%) train and $28$ ($\sim$ 11\%) test samples. The images are labeled according to existence of tumor: 0 for no tumor and 1 for tumor.
    
    \item[]
    \textbf{Radiomics dataset:} This dataset has been automatically generated by extraction of radiomics features such as first-order statistics, texture, and shape from the aforementioned MRI images dataset using an open-source python package called pyradiomics\footnote{\url{https://pyradiomics.readthedocs.io/en/latest/}}. The split and class labels are identical.
    \end{itemize}

\subsection{Models}
We evaluate several interpretable models and post-hoc explanation methods with varying complexity to investigate the trade-off between model complexity, performance, and interpretability and explainability. In the following we outline the models and methods that we use:

\begin{itemize}[leftmargin=0cm]
    \setlength\itemsep{0.6em}
    \item[]
    \textbf{Random Forest:}
    We employ a simple random forest classifier \citep{breiman2001random} using the radiomics features as a baseline that prioritizes performance over interpretability (\textsc{Random Forest}). We add a permutation based post-hoc explanation method \citep{breiman2001random} to aid in visualizing which features were the most important during classification.\footnote{See \url{https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html}.}.\
    

    \item[]
    \textbf{Decision Tree:}
    A decision tree is arguable one of the most interpretable traditional ML models. The model and its training algorithm are easily understood, and we are able to interpret the model by visualizing the splits in each node. We introduce a decision tree model also trained on the radiomics features (\textsc{Decision Tree}) in contrast to the more complex \textsc{Random Forest} classifier and to highlight any potential performance cost of having a more interpretable tree-based model.
    
    \item[]
    \textbf{Logistic Regression:}
    Additionally, we train a simple logistic regression model (\textsc{Logistic Regression}) that lends itself to interpretation. We obtain a clear view of the important features used the model through its linear coefficients and their magnitude. This also allows us to see which features strongly predict or contribute to the tumor or no tumor prediction by their sign and magnitude. To further aid interpretability, we use L1 regularization to force coefficients of unhelpful features to 0.
    
    \item[]
    \textbf{Baseline CNN:}
    We train the provided baseline CNN model for increased accuracy over the simpler models (\textsc{Baseline CNN}). The trade-off we highlight here is model complexity, often leading to better performance, and interpretability. We add two post-hoc explanation methods, SHAP and LIME as described in the following section, to give a better understanding of the model and its predictions.
    
    
    \item[]
    \textbf{VGG16 TL:}
    Finally, we evaluate a VGG16 model with transfer learning (\textsc{VGG16 TL}). The model is pre-trained on the ImageNet dataset. We add three dense layers followed by a dropout layer and finally the output layer. All layers in the original model are frozen and then we fine-tune our added layers on the MRI image dataset. We further employ data augmentation to add some modifications to some of our input images by making minor changes, such as flipping, enhancing their contrast and sharpness leading to increased performance.
\end{itemize}
    
\subsection{Post-hoc Explanation Methods}
\begin{itemize}[leftmargin=0cm]
    \setlength\itemsep{0.6em}
    \item[]
    \textbf{SHAP:}    
    SHapley Additive exPlanations (SHAP) \citep{lundberg2017unified} is a model-agnostic approach that breaks down a prediction to show the impact of each feature. Image classification tasks are explained by attributing scores to each pixel on a predicted image. These scores indicate how much the pixels contribute to the probability of the image being classified in that particular class. Red pixels represent positive SHAP values that contributed positively to the classification of image as a particular class and blue pixels represent negative SHAP values which contributed to not classifying the image as that class.
    
    \item[]
    \textbf{LIME:}
    Local interpretable model-agnostic explanations (LIME) \citep{lime} is, like SHAP, a model-agnostic approach for explainability. LIME explains individual predictions of a classifier by learning an interpretable and faithful model locally around each prediction. In our task of brain tumor detection from MRI images, LIME produces images that explain our classifiers' predictions by highlighting the important region inside the image.
\end{itemize}

